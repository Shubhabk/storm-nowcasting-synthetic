{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab79747-d688-4d0b-a03e-9789094e16b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Analysis for Scenario 3: All Variables are Variable ---\n",
      "--- Training models on the corrected and updated data file ---\n",
      "Successfully loaded 433021 records from 'scenario3_variable_50000.csv'.\n",
      "Nowcasting dataset has 150000 records (first 10 minutes of each storm).\n",
      "Performing advanced feature engineering on initial 10-minute data...\n",
      "Feature engineering complete.\n",
      "Final dataset for training/testing has 50000 unique storms.\n",
      "Features have been scaled using StandardScaler.\n",
      "\n",
      "--- Model Training & Evaluation for Scenario 3 ---\n",
      "  Training Gradient Boosting for Lifetime Hours...\n",
      "  Training Random Forest for Lifetime Hours...\n",
      "  Training XGBoost for Lifetime Hours...\n",
      "  Training Ridge for Lifetime Hours...\n",
      "  Training Gradient Boosting for Peak Rainfall...\n",
      "  Training Random Forest for Peak Rainfall...\n",
      "  Training XGBoost for Peak Rainfall...\n",
      "  Training Ridge for Peak Rainfall...\n",
      "  Training Gradient Boosting for Total Rainfall...\n",
      "  Training Random Forest for Total Rainfall...\n",
      "  Training XGBoost for Total Rainfall...\n",
      "  Training Ridge for Total Rainfall...\n",
      "\n",
      "================================================================================\n",
      "Performance Metrics for Scenario 3 (Nowcasting from 10 min data) with ADVANCED FEATURES\n",
      "================================================================================\n",
      "\n",
      "Target: Lifetime Hours\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.9832 | 0.0361 | 0.0013 | 0.0236 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.9812 | 0.0382 | 0.0015 | 0.0257 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.9834 | 0.0358 | 0.0013 | 0.0234 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.9291 | 0.0741 | 0.0055 | 0.0539 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "\n",
      "Target: Peak Rainfall\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "| Model             |   R-squared |   RMSE |     MSE |    MAE |\n",
      "+===================+=============+========+=========+========+\n",
      "| Gradient Boosting |      0.8618 | 2.7787 |  7.7214 | 1.9845 |\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "| Random Forest     |      0.6288 | 4.5538 | 20.7367 | 3.2862 |\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "| XGBoost           |      0.8603 | 2.7939 |  7.8058 | 2.0000 |\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "| Ridge             |      0.6716 | 4.2836 | 18.3492 | 3.2485 |\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "\n",
      "Target: Total Rainfall\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.8641 | 0.7563 | 0.5720 | 0.4970 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.7661 | 0.9921 | 0.9842 | 0.6736 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.8635 | 0.7580 | 0.5745 | 0.4976 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.6389 | 1.2326 | 1.5192 | 0.8910 |\n",
      "+-------------------+-------------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------------ HELPER FUNCTIONS ------------------\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, target_name):\n",
    "    \"\"\"Trains and evaluates four regression models.\"\"\"\n",
    "    models = {\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "        'XGBoost': XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42),\n",
    "        'Ridge': Ridge(alpha=1.0)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f\"  Training {name} for {target_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        results.append([name, r2, rmse, mse, mae])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------ MAIN SCRIPT FOR SCENARIO 3 (WITH NEW FEATURES) ------------------\n",
    "print(\"--- Starting Analysis for Scenario 3: All Variables are Variable ---\")\n",
    "print(\"--- Training models on the corrected and updated data file ---\")\n",
    "\n",
    "# Step 1: Load the data\n",
    "try:\n",
    "    storm_df = pd.read_csv(\"scenario3_variable_50000.csv\")\n",
    "    storm_df['timestamp_utc'] = pd.to_datetime(storm_df['timestamp_utc'])\n",
    "    print(f\"Successfully loaded {len(storm_df)} records from 'scenario3_variable_50000.csv'.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'scenario3_variable_50000.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Nowcasting-style Data Preparation (First 10 minutes)\n",
    "initial_data_df = storm_df[storm_df['time_since_formation_hours'] <= 10/60]\n",
    "print(f\"Nowcasting dataset has {len(initial_data_df)} records (first 10 minutes of each storm).\")\n",
    "\n",
    "# Step 3: Advanced Feature Engineering\n",
    "print(\"Performing advanced feature engineering on initial 10-minute data...\")\n",
    "\n",
    "# Calculate the slopes and second-order derivatives for each storm's initial phase\n",
    "initial_data_df = initial_data_df.sort_values(by=['cell_id', 'time_since_formation_hours'])\n",
    "advanced_features_df = initial_data_df.groupby('cell_id').apply(\n",
    "    lambda group: pd.Series({\n",
    "        # Simple slopes\n",
    "        'intensity_slope_10min': (group['intensity_dbz'].iloc[-1] - group['intensity_dbz'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        'size_slope_10min': (group['size_pixels'].iloc[-1] - group['size_pixels'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        'rainfall_slope_10min': (group['rainfall_mm_per_hr'].iloc[-1] - group['rainfall_mm_per_hr'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        \n",
    "        # Second-order derivative (change in slope)\n",
    "        'intensity_accel_10min': group['intensity_change_rate'].diff().sum(),\n",
    "        'size_accel_10min': group['size_pixels'].diff().sum(),\n",
    "        'rainfall_accel_10min': group['rainfall_mm_per_hr'].diff().sum(),\n",
    "        \n",
    "        # Statistical measures\n",
    "        'intensity_std_10min': group['intensity_dbz'].std(),\n",
    "        'size_std_10min': group['size_pixels'].std(),\n",
    "        'rainfall_std_10min': group['rainfall_mm_per_hr'].std(),\n",
    "        \n",
    "        # Cumulative metrics (cumulative sum over time)\n",
    "        'cumulative_intensity_10min': group['intensity_dbz'].sum() * (5/60),\n",
    "        'cumulative_rainfall_10min': group['rainfall_mm_per_hr'].sum() * (5/60)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Now, perform the rest of the feature engineering using simple aggregations\n",
    "engineered_features_stats = initial_data_df.groupby('cell_id').agg(\n",
    "    initial_intensity_dbz=('intensity_dbz', 'first'),\n",
    "    initial_size_pixels=('size_pixels', 'first'),\n",
    "    initial_rainfall_mm_per_hr=('rainfall_mm_per_hr', 'first'),\n",
    "    max_intensity_10min=('intensity_dbz', 'max'),\n",
    "    max_size_10min=('size_pixels', 'max'),\n",
    "    max_rainfall_10min=('rainfall_mm_per_hr', 'max'),\n",
    "    mean_intensity_10min=('intensity_dbz', 'mean'),\n",
    "    mean_size_10min=('size_pixels', 'mean'),\n",
    "    mean_rainfall_10min=('rainfall_mm_per_hr', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "# Merge the stats and slopes\n",
    "engineered_features = pd.merge(engineered_features_stats, advanced_features_df, on='cell_id', how='left')\n",
    "print(\"Feature engineering complete.\")\n",
    "\n",
    "# Step 4: Aggregate targets from the full dataset\n",
    "targets_df = storm_df.groupby('cell_id').agg(\n",
    "    lifetime_hours=('lifetime_hours', 'first'),\n",
    "    peak_rainfall_mmhr=('rainfall_mm_per_hr', 'max'),\n",
    "    total_rainfall_mm=('rainfall_mm_per_hr', lambda x: (x * (5/60)).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Merge features with targets\n",
    "dataset = pd.merge(engineered_features, targets_df, on='cell_id', how='inner')\n",
    "dataset.dropna(inplace=True)\n",
    "print(f\"Final dataset for training/testing has {len(dataset)} unique storms.\")\n",
    "\n",
    "# Step 6: Split the data\n",
    "cell_ids = dataset['cell_id'].unique()\n",
    "train_ids, test_ids = train_test_split(cell_ids, test_size=20000, train_size=30000, random_state=42, shuffle=True)\n",
    "\n",
    "train_df = dataset[dataset['cell_id'].isin(train_ids)]\n",
    "test_df = dataset[dataset['cell_id'].isin(test_ids)]\n",
    "\n",
    "X_train = train_df.drop(['cell_id', 'lifetime_hours', 'peak_rainfall_mmhr', 'total_rainfall_mm'], axis=1)\n",
    "y_train_lifetime = train_df['lifetime_hours']\n",
    "y_train_peak_rainfall = train_df['peak_rainfall_mmhr']\n",
    "y_train_total_rainfall = train_df['total_rainfall_mm']\n",
    "\n",
    "X_test = test_df.drop(['cell_id', 'lifetime_hours', 'peak_rainfall_mmhr', 'total_rainfall_mm'], axis=1)\n",
    "y_test_lifetime = test_df['lifetime_hours']\n",
    "y_test_peak_rainfall = test_df['peak_rainfall_mmhr']\n",
    "y_test_total_rainfall = test_df['total_rainfall_mm']\n",
    "\n",
    "# Step 7: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Features have been scaled using StandardScaler.\")\n",
    "\n",
    "# Step 8: Train and Evaluate models for each target\n",
    "print(\"\\n--- Model Training & Evaluation for Scenario 3 ---\")\n",
    "all_results = {}\n",
    "all_results['Lifetime Hours'] = train_and_evaluate_models(X_train_scaled, y_train_lifetime, X_test_scaled, y_test_lifetime, 'Lifetime Hours')\n",
    "all_results['Peak Rainfall'] = train_and_evaluate_models(X_train_scaled, y_train_peak_rainfall, X_test_scaled, y_test_peak_rainfall, 'Peak Rainfall')\n",
    "all_results['Total Rainfall'] = train_and_evaluate_models(X_train_scaled, y_train_total_rainfall, X_test_scaled, y_test_total_rainfall, 'Total Rainfall')\n",
    "\n",
    "# Step 9: Print results in a single table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Performance Metrics for Scenario 3 (Nowcasting from 10 min data) with ADVANCED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "for target, results in all_results.items():\n",
    "    print(f\"\\nTarget: {target}\")\n",
    "    headers = [\"Model\", \"R-squared\", \"RMSE\", \"MSE\", \"MAE\"]\n",
    "    print(tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe58726-d711-46c3-b892-a2f432794c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab7e9466-0513-494e-a4ea-2766b46a94bd",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6aeaab8-997b-4ec6-817d-84cffa03e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Analysis for Scenario 3: All Variables are Variable ---\n",
      "--- Training models on the corrected and updated data file ---\n",
      "Successfully loaded 433021 records from 'scenario3_variable_50000.csv'.\n",
      "Nowcasting dataset has 150000 records (first 10 minutes of each storm).\n",
      "Performing advanced feature engineering on initial 10-minute data...\n",
      "Feature engineering complete.\n",
      "Final dataset for training/testing has 50000 unique storms.\n",
      "Features have been scaled using StandardScaler.\n",
      "\n",
      "--- Model Training & Evaluation for Scenario 3 ---\n",
      "  Training Gradient Boosting for Lifetime Hours...\n",
      "  Training Random Forest for Lifetime Hours...\n",
      "  Training XGBoost for Lifetime Hours...\n",
      "  Training Ridge for Lifetime Hours...\n",
      "  Training Gradient Boosting for Peak Rainfall...\n",
      "  Training Random Forest for Peak Rainfall...\n",
      "  Training XGBoost for Peak Rainfall...\n",
      "  Training Ridge for Peak Rainfall...\n",
      "  Training Gradient Boosting for Total Rainfall...\n",
      "  Training Random Forest for Total Rainfall...\n",
      "  Training XGBoost for Total Rainfall...\n",
      "  Training Ridge for Total Rainfall...\n",
      "\n",
      "================================================================================\n",
      "Performance Metrics for Scenario 3 (Nowcasting from 10 min data) with ADVANCED FEATURES\n",
      "================================================================================\n",
      "\n",
      "Target: Lifetime Hours\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.9832 | 0.0361 | 0.0013 | 0.0236 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.9812 | 0.0382 | 0.0015 | 0.0257 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.9834 | 0.0358 | 0.0013 | 0.0234 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.9291 | 0.0741 | 0.0055 | 0.0539 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "\n",
      "Target: Peak Rainfall\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "| Model             |   R-squared |   RMSE |     MSE |    MAE |\n",
      "+===================+=============+========+=========+========+\n",
      "| Gradient Boosting |      0.8618 | 2.7787 |  7.7214 | 1.9845 |\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "| Random Forest     |      0.6288 | 4.5538 | 20.7367 | 3.2862 |\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "| XGBoost           |      0.8603 | 2.7939 |  7.8058 | 2.0000 |\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "| Ridge             |      0.6716 | 4.2836 | 18.3492 | 3.2485 |\n",
      "+-------------------+-------------+--------+---------+--------+\n",
      "\n",
      "Target: Total Rainfall\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.8641 | 0.7563 | 0.5720 | 0.4970 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.7661 | 0.9921 | 0.9842 | 0.6736 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.8635 | 0.7580 | 0.5745 | 0.4976 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.6389 | 1.2326 | 1.5192 | 0.8910 |\n",
      "+-------------------+-------------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------------ HELPER FUNCTIONS ------------------\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, target_name):\n",
    "    \"\"\"Trains and evaluates four regression models.\"\"\"\n",
    "    models = {\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "        'XGBoost': XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42),\n",
    "        'Ridge': Ridge(alpha=1.0)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f\"  Training {name} for {target_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        results.append([name, r2, rmse, mse, mae])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------ MAIN SCRIPT FOR SCENARIO 3 (WITH NEW FEATURES) ------------------\n",
    "print(\"--- Starting Analysis for Scenario 3: All Variables are Variable ---\")\n",
    "print(\"--- Training models on the corrected and updated data file ---\")\n",
    "\n",
    "# Step 1: Load the data\n",
    "try:\n",
    "    storm_df = pd.read_csv(\"scenario3_variable_50000.csv\")\n",
    "    storm_df['timestamp_utc'] = pd.to_datetime(storm_df['timestamp_utc'])\n",
    "    print(f\"Successfully loaded {len(storm_df)} records from 'scenario3_variable_50000.csv'.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'scenario3_variable_50000.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Nowcasting-style Data Preparation (First 10 minutes)\n",
    "initial_data_df = storm_df[storm_df['time_since_formation_hours'] <= 10/60]\n",
    "print(f\"Nowcasting dataset has {len(initial_data_df)} records (first 10 minutes of each storm).\")\n",
    "\n",
    "# Step 3: Advanced Feature Engineering\n",
    "print(\"Performing advanced feature engineering on initial 10-minute data...\")\n",
    "\n",
    "# Calculate the slopes and second-order derivatives for each storm's initial phase\n",
    "initial_data_df = initial_data_df.sort_values(by=['cell_id', 'time_since_formation_hours'])\n",
    "advanced_features_df = initial_data_df.groupby('cell_id').apply(\n",
    "    lambda group: pd.Series({\n",
    "        # Simple slopes\n",
    "        'intensity_slope_10min': (group['intensity_dbz'].iloc[-1] - group['intensity_dbz'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        'size_slope_10min': (group['size_pixels'].iloc[-1] - group['size_pixels'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        'rainfall_slope_10min': (group['rainfall_mm_per_hr'].iloc[-1] - group['rainfall_mm_per_hr'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        \n",
    "        # Second-order derivative (change in slope)\n",
    "        'intensity_accel_10min': group['intensity_change_rate'].diff().sum(),\n",
    "        'size_accel_10min': group['size_pixels'].diff().sum(),\n",
    "        'rainfall_accel_10min': group['rainfall_mm_per_hr'].diff().sum(),\n",
    "        \n",
    "        # Statistical measures\n",
    "        'intensity_std_10min': group['intensity_dbz'].std(),\n",
    "        'size_std_10min': group['size_pixels'].std(),\n",
    "        'rainfall_std_10min': group['rainfall_mm_per_hr'].std(),\n",
    "        \n",
    "        # Cumulative metrics (cumulative sum over time)\n",
    "        'cumulative_intensity_10min': group['intensity_dbz'].sum() * (5/60),\n",
    "        'cumulative_rainfall_10min': group['rainfall_mm_per_hr'].sum() * (5/60)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Now, perform the rest of the feature engineering using simple aggregations\n",
    "engineered_features_stats = initial_data_df.groupby('cell_id').agg(\n",
    "    initial_intensity_dbz=('intensity_dbz', 'first'),\n",
    "    initial_size_pixels=('size_pixels', 'first'),\n",
    "    initial_rainfall_mm_per_hr=('rainfall_mm_per_hr', 'first'),\n",
    "    max_intensity_10min=('intensity_dbz', 'max'),\n",
    "    max_size_10min=('size_pixels', 'max'),\n",
    "    max_rainfall_10min=('rainfall_mm_per_hr', 'max'),\n",
    "    mean_intensity_10min=('intensity_dbz', 'mean'),\n",
    "    mean_size_10min=('size_pixels', 'mean'),\n",
    "    mean_rainfall_10min=('rainfall_mm_per_hr', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "# Merge the stats and slopes\n",
    "engineered_features = pd.merge(engineered_features_stats, advanced_features_df, on='cell_id', how='left')\n",
    "print(\"Feature engineering complete.\")\n",
    "\n",
    "# Step 4: Aggregate targets from the full dataset\n",
    "targets_df = storm_df.groupby('cell_id').agg(\n",
    "    lifetime_hours=('lifetime_hours', 'first'),\n",
    "    peak_rainfall_mmhr=('rainfall_mm_per_hr', 'max'),\n",
    "    total_rainfall_mm=('rainfall_mm_per_hr', lambda x: (x * (5/60)).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Merge features with targets\n",
    "dataset = pd.merge(engineered_features, targets_df, on='cell_id', how='inner')\n",
    "dataset.dropna(inplace=True)\n",
    "print(f\"Final dataset for training/testing has {len(dataset)} unique storms.\")\n",
    "\n",
    "# Step 6: Split the data\n",
    "cell_ids = dataset['cell_id'].unique()\n",
    "train_ids, test_ids = train_test_split(cell_ids, test_size=20000, train_size=30000, random_state=42, shuffle=True)\n",
    "\n",
    "train_df = dataset[dataset['cell_id'].isin(train_ids)]\n",
    "test_df = dataset[dataset['cell_id'].isin(test_ids)]\n",
    "\n",
    "X_train = train_df.drop(['cell_id', 'lifetime_hours', 'peak_rainfall_mmhr', 'total_rainfall_mm'], axis=1)\n",
    "y_train_lifetime = train_df['lifetime_hours']\n",
    "y_train_peak_rainfall = train_df['peak_rainfall_mmhr']\n",
    "y_train_total_rainfall = train_df['total_rainfall_mm']\n",
    "\n",
    "X_test = test_df.drop(['cell_id', 'lifetime_hours', 'peak_rainfall_mmhr', 'total_rainfall_mm'], axis=1)\n",
    "y_test_lifetime = test_df['lifetime_hours']\n",
    "y_test_peak_rainfall = test_df['peak_rainfall_mmhr']\n",
    "y_test_total_rainfall = test_df['total_rainfall_mm']\n",
    "\n",
    "# Step 7: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Features have been scaled using StandardScaler.\")\n",
    "\n",
    "# Step 8: Train and Evaluate models for each target\n",
    "print(\"\\n--- Model Training & Evaluation for Scenario 3 ---\")\n",
    "all_results = {}\n",
    "all_results['Lifetime Hours'] = train_and_evaluate_models(X_train_scaled, y_train_lifetime, X_test_scaled, y_test_lifetime, 'Lifetime Hours')\n",
    "all_results['Peak Rainfall'] = train_and_evaluate_models(X_train_scaled, y_train_peak_rainfall, X_test_scaled, y_test_peak_rainfall, 'Peak Rainfall')\n",
    "all_results['Total Rainfall'] = train_and_evaluate_models(X_train_scaled, y_train_total_rainfall, X_test_scaled, y_test_total_rainfall, 'Total Rainfall')\n",
    "\n",
    "# Step 9: Print results in a single table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Performance Metrics for Scenario 3 (Nowcasting from 10 min data) with ADVANCED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "for target, results in all_results.items():\n",
    "    print(f\"\\nTarget: {target}\")\n",
    "    headers = [\"Model\", \"R-squared\", \"RMSE\", \"MSE\", \"MAE\"]\n",
    "    print(tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de6af5-1934-42f8-bc4a-4532120a78df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
