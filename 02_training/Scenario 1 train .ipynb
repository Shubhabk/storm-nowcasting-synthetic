{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea48fa7-76f1-4402-9d1c-46ffb9378201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Analysis for Scenario 1: Fixed Intensity for all Storm Cells ---\n",
      "Successfully loaded 435202 records from 'scenario6_fixed_peak_50000.csv'.\n",
      "Nowcasting dataset has 150000 records (first 10 minutes of each storm).\n",
      "Performing feature engineering on initial 10-minute data...\n",
      "Feature engineering complete.\n",
      "Final dataset for training/testing has 50000 unique storms.\n",
      "Features have been scaled using StandardScaler.\n",
      "\n",
      "--- Model Training & Evaluation for Scenario 1 ---\n",
      "  Training Gradient Boosting for Lifetime Hours...\n",
      "  Training Random Forest for Lifetime Hours...\n",
      "  Training XGBoost for Lifetime Hours...\n",
      "  Training Ridge for Lifetime Hours...\n",
      "  Training Gradient Boosting for Peak Rainfall...\n",
      "  Training Random Forest for Peak Rainfall...\n",
      "  Training XGBoost for Peak Rainfall...\n",
      "  Training Ridge for Peak Rainfall...\n",
      "  Training Gradient Boosting for Total Rainfall...\n",
      "  Training Random Forest for Total Rainfall...\n",
      "  Training XGBoost for Total Rainfall...\n",
      "  Training Ridge for Total Rainfall...\n",
      "\n",
      "================================================================================\n",
      "Performance Metrics for Scenario 1 (Nowcasting from 10 min data)\n",
      "================================================================================\n",
      "\n",
      "Target: Lifetime Hours\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.9999 | 0.0022 | 0.0000 | 0.0016 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.9992 | 0.0079 | 0.0001 | 0.0062 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.9999 | 0.0031 | 0.0000 | 0.0019 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.7743 | 0.1320 | 0.0174 | 0.1034 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "\n",
      "Target: Peak Rainfall\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.2000 | 2.6906 | 7.2391 | 2.0954 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.2002 | 2.6902 | 7.2374 | 2.1084 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.1997 | 2.6911 | 7.2420 | 2.0961 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.1115 | 2.8356 | 8.0406 | 2.3718 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "\n",
      "Target: Total Rainfall\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.8898 | 0.2295 | 0.0527 | 0.1791 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.8849 | 0.2345 | 0.0550 | 0.1905 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.8893 | 0.2299 | 0.0529 | 0.1793 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.6316 | 0.4195 | 0.1760 | 0.3240 |\n",
      "+-------------------+-------------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------------ HELPER FUNCTIONS ------------------\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, target_name):\n",
    "    \"\"\"Trains and evaluates four regression models.\"\"\"\n",
    "    models = {\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "        'XGBoost': XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42),\n",
    "        'Ridge': Ridge(alpha=1.0)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f\"  Training {name} for {target_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        results.append([name, r2, rmse, mse, mae])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------ MAIN SCRIPT FOR SCENARIO 1 ------------------\n",
    "print(\"--- Starting Analysis for Scenario 1: Fixed Intensity for all Storm Cells ---\")\n",
    "\n",
    "# Step 1: Load the data\n",
    "try:\n",
    "    storm_df = pd.read_csv(\"scenario6_fixed_peak_50000.csv\")\n",
    "    storm_df['timestamp_utc'] = pd.to_datetime(storm_df['timestamp_utc'])\n",
    "    print(f\"Successfully loaded {len(storm_df)} records from 'scenario6_fixed_peak_50000.csv'.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'scenario6_fixed_peak_50000.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Nowcasting-style Data Preparation (First 10 minutes)\n",
    "initial_data_df = storm_df[storm_df['time_since_formation_hours'] <= 10/60]\n",
    "print(f\"Nowcasting dataset has {len(initial_data_df)} records (first 10 minutes of each storm).\")\n",
    "\n",
    "# Step 3: Corrected Feature Engineering\n",
    "print(\"Performing feature engineering on initial 10-minute data...\")\n",
    "\n",
    "# Calculate the slopes for each storm's initial phase\n",
    "initial_data_df = initial_data_df.sort_values(by=['cell_id', 'time_since_formation_hours'])\n",
    "slopes_df = initial_data_df.groupby('cell_id').apply(\n",
    "    lambda group: pd.Series({\n",
    "        'intensity_slope': (group['intensity_dbz'].iloc[-1] - group['intensity_dbz'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        'size_slope': (group['size_pixels'].iloc[-1] - group['size_pixels'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        'rainfall_slope': (group['rainfall_mm_per_hr'].iloc[-1] - group['rainfall_mm_per_hr'].iloc[0]) / (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0]) if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Now, perform the rest of the feature engineering using simple aggregations\n",
    "engineered_features_stats = initial_data_df.groupby('cell_id').agg(\n",
    "    initial_intensity_dbz=('intensity_dbz', 'first'),\n",
    "    initial_size_pixels=('size_pixels', 'first'),\n",
    "    initial_rainfall_mm_per_hr=('rainfall_mm_per_hr', 'first'),\n",
    "    max_intensity_10min=('intensity_dbz', 'max'),\n",
    "    max_size_10min=('size_pixels', 'max'),\n",
    "    max_rainfall_10min=('rainfall_mm_per_hr', 'max'),\n",
    "    mean_intensity_10min=('intensity_dbz', 'mean'),\n",
    "    mean_size_10min=('size_pixels', 'mean'),\n",
    "    mean_rainfall_10min=('rainfall_mm_per_hr', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "# Merge the stats and slopes\n",
    "engineered_features = pd.merge(engineered_features_stats, slopes_df, on='cell_id', how='left')\n",
    "print(\"Feature engineering complete.\")\n",
    "\n",
    "# Step 4: Aggregate targets from the full dataset\n",
    "targets_df = storm_df.groupby('cell_id').agg(\n",
    "    lifetime_hours=('lifetime_hours', 'first'),\n",
    "    peak_rainfall_mmhr=('rainfall_mm_per_hr', 'max'),\n",
    "    total_rainfall_mm=('rainfall_mm_per_hr', lambda x: (x * (5/60)).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Merge features with targets\n",
    "dataset = pd.merge(engineered_features, targets_df, on='cell_id', how='inner')\n",
    "dataset.dropna(inplace=True)\n",
    "print(f\"Final dataset for training/testing has {len(dataset)} unique storms.\")\n",
    "\n",
    "# Step 6: Split the data\n",
    "cell_ids = dataset['cell_id'].unique()\n",
    "train_ids, test_ids = train_test_split(cell_ids, test_size=20000, train_size=30000, random_state=42, shuffle=True)\n",
    "\n",
    "train_df = dataset[dataset['cell_id'].isin(train_ids)]\n",
    "test_df = dataset[dataset['cell_id'].isin(test_ids)]\n",
    "\n",
    "X_train = train_df.drop(['cell_id', 'lifetime_hours', 'peak_rainfall_mmhr', 'total_rainfall_mm'], axis=1)\n",
    "y_train_lifetime = train_df['lifetime_hours']\n",
    "y_train_peak_rainfall = train_df['peak_rainfall_mmhr']\n",
    "y_train_total_rainfall = train_df['total_rainfall_mm']\n",
    "\n",
    "X_test = test_df.drop(['cell_id', 'lifetime_hours', 'peak_rainfall_mmhr', 'total_rainfall_mm'], axis=1)\n",
    "y_test_lifetime = test_df['lifetime_hours']\n",
    "y_test_peak_rainfall = test_df['peak_rainfall_mmhr']\n",
    "y_test_total_rainfall = test_df['total_rainfall_mm']\n",
    "\n",
    "# Step 7: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Features have been scaled using StandardScaler.\")\n",
    "\n",
    "# Step 8: Train and Evaluate models for each target\n",
    "print(\"\\n--- Model Training & Evaluation for Scenario 1 ---\")\n",
    "all_results = {}\n",
    "all_results['Lifetime Hours'] = train_and_evaluate_models(X_train_scaled, y_train_lifetime, X_test_scaled, y_test_lifetime, 'Lifetime Hours')\n",
    "all_results['Peak Rainfall'] = train_and_evaluate_models(X_train_scaled, y_train_peak_rainfall, X_test_scaled, y_test_peak_rainfall, 'Peak Rainfall')\n",
    "all_results['Total Rainfall'] = train_and_evaluate_models(X_train_scaled, y_train_total_rainfall, X_test_scaled, y_test_total_rainfall, 'Total Rainfall')\n",
    "\n",
    "# Step 9: Print results in a single table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Performance Metrics for Scenario 1 (Nowcasting from 10 min data)\")\n",
    "print(\"=\"*80)\n",
    "for target, results in all_results.items():\n",
    "    print(f\"\\nTarget: {target}\")\n",
    "    headers = [\"Model\", \"R-squared\", \"RMSE\", \"MSE\", \"MAE\"]\n",
    "    print(tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd312b-28c1-446c-8b7e-0d845fd738a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b032e2a0-7940-4e49-a278-5cac27ed904e",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37a33a2-9362-4671-8300-1aa45e98dbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Analysis for Scenario 1: Fixed Intensity for all Storm Cells ---\n",
      "Successfully loaded 435202 records from 'scenario6_fixed_peak_50000.csv'.\n",
      "Nowcasting dataset has 150000 records (first 10 minutes of each storm).\n",
      "Performing feature engineering on initial 10-minute data...\n",
      "Feature engineering complete.\n",
      "Final dataset for training/testing has 50000 unique storms.\n",
      "Features have been scaled using StandardScaler.\n",
      "\n",
      "--- Model Training & Evaluation for Scenario 1 ---\n",
      "  Training Gradient Boosting for Lifetime Hours...\n",
      "  Training Random Forest for Lifetime Hours...\n",
      "  Training XGBoost for Lifetime Hours...\n",
      "  Training Ridge for Lifetime Hours...\n",
      "  Training Gradient Boosting for Peak Rainfall...\n",
      "  Training Random Forest for Peak Rainfall...\n",
      "  Training XGBoost for Peak Rainfall...\n",
      "  Training Ridge for Peak Rainfall...\n",
      "  Training Gradient Boosting for Total Rainfall...\n",
      "  Training Random Forest for Total Rainfall...\n",
      "  Training XGBoost for Total Rainfall...\n",
      "  Training Ridge for Total Rainfall...\n",
      "\n",
      "================================================================================\n",
      "Performance Metrics for Scenario 1 (Nowcasting from 10 min data)\n",
      "================================================================================\n",
      "\n",
      "Target: Lifetime Hours\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      1.0000 | 0.0006 | 0.0000 | 0.0004 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      1.0000 | 0.0006 | 0.0000 | 0.0004 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.9999 | 0.0021 | 0.0000 | 0.0007 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.9039 | 0.0861 | 0.0074 | 0.0632 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "\n",
      "Target: Peak Rainfall\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.1961 | 2.6971 | 7.2744 | 2.0965 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.2006 | 2.6896 | 7.2337 | 2.0858 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.1948 | 2.6993 | 7.2862 | 2.1019 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.1974 | 2.6950 | 7.2633 | 2.1254 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "\n",
      "Target: Total Rainfall\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Model             |   R-squared |   RMSE |    MSE |    MAE |\n",
      "+===================+=============+========+========+========+\n",
      "| Gradient Boosting |      0.8900 | 0.2292 | 0.0525 | 0.1786 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Random Forest     |      0.8901 | 0.2291 | 0.0525 | 0.1803 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| XGBoost           |      0.8895 | 0.2298 | 0.0528 | 0.1784 |\n",
      "+-------------------+-------------+--------+--------+--------+\n",
      "| Ridge             |      0.7109 | 0.3716 | 0.1381 | 0.2736 |\n",
      "+-------------------+-------------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------------ HELPER FUNCTIONS ------------------\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, target_name):\n",
    "    \"\"\"Trains and evaluates four regression models.\"\"\"\n",
    "    models = {\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=300, learning_rate=0.05, max_depth=4, random_state=42\n",
    "        ),\n",
    "        'Random Forest': RandomForestRegressor(\n",
    "            n_estimators=300, max_depth=7, random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=1000, learning_rate=0.01, max_depth=6,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'Ridge': Ridge(alpha=1.0)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f\"  Training {name} for {target_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        results.append([name, r2, rmse, mse, mae])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------ MAIN SCRIPT ------------------\n",
    "print(\"--- Starting Analysis for Scenario 1: Fixed Intensity for all Storm Cells ---\")\n",
    "\n",
    "# Step 1: Load the data\n",
    "try:\n",
    "    storm_df = pd.read_csv(\"scenario6_fixed_peak_50000.csv\")\n",
    "    storm_df['timestamp_utc'] = pd.to_datetime(storm_df['timestamp_utc'])\n",
    "    print(f\"Successfully loaded {len(storm_df)} records from 'scenario6_fixed_peak_50000.csv'.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'scenario6_fixed_peak_50000.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Nowcasting-style Data Preparation (First 10 minutes)\n",
    "initial_data_df = storm_df[storm_df['time_since_formation_hours'] <= 10/60]\n",
    "print(f\"Nowcasting dataset has {len(initial_data_df)} records (first 10 minutes of each storm).\")\n",
    "\n",
    "# Step 3: Feature Engineering\n",
    "print(\"Performing feature engineering on initial 10-minute data...\")\n",
    "\n",
    "# Slopes\n",
    "initial_data_df = initial_data_df.sort_values(by=['cell_id', 'time_since_formation_hours'])\n",
    "slopes_df = initial_data_df.groupby('cell_id').apply(\n",
    "    lambda group: pd.Series({\n",
    "        'intensity_slope': (group['intensity_dbz'].iloc[-1] - group['intensity_dbz'].iloc[0]) /\n",
    "                           (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0])\n",
    "                           if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        'size_slope': (group['size_pixels'].iloc[-1] - group['size_pixels'].iloc[0]) /\n",
    "                      (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0])\n",
    "                      if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0,\n",
    "        'rainfall_slope': (group['rainfall_mm_per_hr'].iloc[-1] - group['rainfall_mm_per_hr'].iloc[0]) /\n",
    "                          (group['time_since_formation_hours'].iloc[-1] - group['time_since_formation_hours'].iloc[0])\n",
    "                          if len(group) > 1 and group['time_since_formation_hours'].iloc[-1] != group['time_since_formation_hours'].iloc[0] else 0\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Aggregations\n",
    "engineered_features_stats = initial_data_df.groupby('cell_id').agg(\n",
    "    initial_intensity_dbz=('intensity_dbz', 'first'),\n",
    "    initial_size_pixels=('size_pixels', 'first'),\n",
    "    initial_rainfall_mm_per_hr=('rainfall_mm_per_hr', 'first'),\n",
    "    max_intensity_10min=('intensity_dbz', 'max'),\n",
    "    max_size_10min=('size_pixels', 'max'),\n",
    "    max_rainfall_10min=('rainfall_mm_per_hr', 'max'),\n",
    "    mean_intensity_10min=('intensity_dbz', 'mean'),\n",
    "    mean_size_10min=('size_pixels', 'mean'),\n",
    "    mean_rainfall_10min=('rainfall_mm_per_hr', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "# Volatility features\n",
    "vol_features = initial_data_df.groupby('cell_id').agg(\n",
    "    std_intensity_10min=('intensity_dbz', 'std'),\n",
    "    std_size_10min=('size_pixels', 'std'),\n",
    "    std_rainfall_10min=('rainfall_mm_per_hr', 'std'),\n",
    "    min_intensity_10min=('intensity_dbz', 'min'),\n",
    "    min_rainfall_10min=('rainfall_mm_per_hr', 'min')\n",
    ").reset_index()\n",
    "\n",
    "# Merge slopes + stats + volatility\n",
    "engineered_features = pd.merge(engineered_features_stats, slopes_df, on='cell_id', how='left')\n",
    "engineered_features = pd.merge(engineered_features, vol_features, on='cell_id', how='left')\n",
    "\n",
    "# Growth ratios\n",
    "engineered_features['rainfall_growth_ratio'] = (\n",
    "    engineered_features['max_rainfall_10min'] / engineered_features['initial_rainfall_mm_per_hr']\n",
    ").replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "engineered_features['size_growth_ratio'] = (\n",
    "    engineered_features['max_size_10min'] / engineered_features['initial_size_pixels']\n",
    ").replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Interaction features\n",
    "engineered_features['intensity_x_size'] = (\n",
    "    engineered_features['mean_intensity_10min'] * engineered_features['mean_size_10min']\n",
    ")\n",
    "engineered_features['rainfall_x_size'] = (\n",
    "    engineered_features['mean_rainfall_10min'] * engineered_features['mean_size_10min']\n",
    ")\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "\n",
    "# Step 4: Targets\n",
    "targets_df = storm_df.groupby('cell_id').agg(\n",
    "    lifetime_hours=('lifetime_hours', 'first'),\n",
    "    peak_rainfall_mmhr=('rainfall_mm_per_hr', 'max'),\n",
    "    total_rainfall_mm=('rainfall_mm_per_hr', lambda x: (x * (5/60)).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Merge features + targets\n",
    "dataset = pd.merge(engineered_features, targets_df, on='cell_id', how='inner')\n",
    "dataset.dropna(inplace=True)\n",
    "print(f\"Final dataset for training/testing has {len(dataset)} unique storms.\")\n",
    "\n",
    "# Step 6: Split data\n",
    "cell_ids = dataset['cell_id'].unique()\n",
    "train_ids, test_ids = train_test_split(cell_ids, test_size=20000, train_size=30000, random_state=42, shuffle=True)\n",
    "\n",
    "train_df = dataset[dataset['cell_id'].isin(train_ids)]\n",
    "test_df = dataset[dataset['cell_id'].isin(test_ids)]\n",
    "\n",
    "X_train = train_df.drop(['cell_id', 'lifetime_hours', 'peak_rainfall_mmhr', 'total_rainfall_mm'], axis=1)\n",
    "y_train_lifetime = train_df['lifetime_hours']\n",
    "y_train_peak_rainfall = train_df['peak_rainfall_mmhr']\n",
    "y_train_total_rainfall = train_df['total_rainfall_mm']\n",
    "\n",
    "X_test = test_df.drop(['cell_id', 'lifetime_hours', 'peak_rainfall_mmhr', 'total_rainfall_mm'], axis=1)\n",
    "y_test_lifetime = test_df['lifetime_hours']\n",
    "y_test_peak_rainfall = test_df['peak_rainfall_mmhr']\n",
    "y_test_total_rainfall = test_df['total_rainfall_mm']\n",
    "\n",
    "# Step 7: Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Features have been scaled using StandardScaler.\")\n",
    "\n",
    "# Step 8: Train and Evaluate\n",
    "print(\"\\n--- Model Training & Evaluation for Scenario 1 ---\")\n",
    "all_results = {}\n",
    "all_results['Lifetime Hours'] = train_and_evaluate_models(X_train_scaled, y_train_lifetime, X_test_scaled, y_test_lifetime, 'Lifetime Hours')\n",
    "all_results['Peak Rainfall'] = train_and_evaluate_models(X_train_scaled, y_train_peak_rainfall, X_test_scaled, y_test_peak_rainfall, 'Peak Rainfall')\n",
    "all_results['Total Rainfall'] = train_and_evaluate_models(X_train_scaled, y_train_total_rainfall, X_test_scaled, y_test_total_rainfall, 'Total Rainfall')\n",
    "\n",
    "# Step 9: Print results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Performance Metrics for Scenario 1 (Nowcasting from 10 min data)\")\n",
    "print(\"=\"*80)\n",
    "for target, results in all_results.items():\n",
    "    print(f\"\\nTarget: {target}\")\n",
    "    headers = [\"Model\", \"R-squared\", \"RMSE\", \"MSE\", \"MAE\"]\n",
    "    print(tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c4a15-0d6a-4276-91ad-c51016510473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
